{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IoULoss(nn.Module) :\n",
    "    def __init__(self, method = 'IoU') :\n",
    "        super().__init__()\n",
    "        self.method = method\n",
    "\n",
    "    def forward(self, inp, target) :\n",
    "        '''\n",
    "        input : (B, # of bboxes, 6)\n",
    "        '''\n",
    "        inp_w = inp[..., 2:3]\n",
    "        inp_h = inp[..., 3:4]\n",
    "        target_w = target[..., 2:3]\n",
    "        target_h = target[..., 3:4]\n",
    "\n",
    "        inp_area = inp_w * inp_h\n",
    "        target_area = target_w * target_h\n",
    "\n",
    "        inp_xmin = inp[..., 0:1] - inp_w / 2\n",
    "        inp_ymin = inp[..., 1:2] - inp_h / 2\n",
    "        inp_xmax = inp[..., 0:1] + inp_w / 2\n",
    "        inp_ymax = inp[..., 1:2] + inp_w / 2\n",
    "\n",
    "        target_xmin = target[..., 0:1] - target_w / 2\n",
    "        target_ymin = target[..., 1:2] - target_h / 2\n",
    "        target_xmax = target[..., 0:1] + target_w / 2\n",
    "        target_ymax = target[..., 1:2] + target_w / 2\n",
    "        \n",
    "        inp_topleft = torch.cat([inp_xmin, inp_ymin], axis = -1)\n",
    "        target_topleft = torch.cat([target_xmin, target_ymin], axis = -1)\n",
    "\n",
    "        inp_bottomright = torch.cat([inp_xmax, inp_ymax], axis = -1)\n",
    "        target_bottomright = torch.cat([target_xmax, target_ymax], axis = -1)\n",
    "\n",
    "        intersection_top_left = torch.max(inp_topleft, target_topleft)\n",
    "        intersection_bottom_right = torch.min(inp_bottomright, target_bottomright)\n",
    "\n",
    "\n",
    "\n",
    "        area_inter = torch.prod(\n",
    "            torch.clip(intersection_bottom_right - intersection_top_left, min = 0 , max = None), -1).unsqueeze(-1)\n",
    "\n",
    "        iou = area_inter / (inp_area + target_area - area_inter + 1e-9)\n",
    "\n",
    "        # GIoU : IoU - |C \\ (A U B)| over C. C는 bbox와 GT를 모두 포함하는 최소 크기의 박스.\n",
    "        C_top_left = torch.min(inp_topleft, target_topleft)\n",
    "        C_bottom_right = torch.max(inp_bottomright, target_bottomright)\n",
    "        C_area = torch.prod(C_bottom_right - C_top_left, -1).unsqueeze(-1)\n",
    "\n",
    "        # DIoU : 중심좌표 반영. 1 - IoU + euclidean(pred_center, gt_center) / (diagonal length of C)**2 . C는 bbox와 GT를 모두 포함하는 최소 크기의 박스.\n",
    "        euclidean = torch.sqrt(torch.sum((inp[..., 0:2] - target[..., 0:2]) ** 2, dim = -1)).unsqueeze(-1)\n",
    "        diagonal_length_C = torch.sum((C_bottom_right - C_top_left) ** 2, dim = -1).unsqueeze(-1)\n",
    "\n",
    "        # CIoU : overlap area, central point distance, aspect ratio 고려. \n",
    "        # 1 - IoU + 1 - IoU + euclidean(pred_center, gt_center) / (diagonal length of C)**2 + aspect_ratio_resemblance * alpha\n",
    "        # aspect_ratio_resemblance = 4 / pi**2 (arctan(w_gt/h_gt) - arctan(w_pred/h_pred)) ** 2. \n",
    "        # (4/pi**2) * (arctan(w/h)) range from -0.5 to 0.5\n",
    "        # alpha = positive trade-off parameter. aspect_ratio_resemblance / (1-IoU) + aspect_ratio_resemblance. IoU가 클수록 aspect_ratio_resemblance의 영향력을 키운다.\n",
    "        aspect_ratio_resemblance = (4 / torch.pi ** 2) * (torch.atan(target_w / target_h) - torch.atan(inp_w / inp_h)) ** 2\n",
    "        alpha = aspect_ratio_resemblance / ( (1 - iou) + aspect_ratio_resemblance)\n",
    "\n",
    "        if self.method == 'IoU' : \n",
    "            return 1 - iou\n",
    "        elif self.method == 'GIoU' :\n",
    "            return 1 - (iou - (C_area - (inp_area + target_area - area_inter)) / C_area)\n",
    "        elif self.method == 'DIoU' :\n",
    "            return 1 - iou + (euclidean / diagonal_length_C)\n",
    "        elif self.method == 'CIoU' :\n",
    "            return 1 - iou + (euclidean / diagonal_length_C) + alpha * aspect_ratio_resemblance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case 1. IoU 0.36\n",
    "# test case 2. IoU 1.\n",
    "# test case 3. IoU 0\n",
    "# test case 4. IoU 0\n",
    "pred = torch.tensor([[0.5, 0.5, 0.5, 0.5],\n",
    "                     [0.5, 0.5, 0.3, 0.3],\n",
    "                     [0.1, 0.3, 0.2, 0.2],\n",
    "                     [0.1, 0.3, 0.2, 0.2]\n",
    "                ]).unsqueeze(0) # for batch\n",
    "\n",
    "gt = torch.tensor([[0.7, 0.7, 0.4, 0.4],\n",
    "                   [0.5, 0.5, 0.3, 0.3],\n",
    "                   [0.6, 0.3, 0.2, 0.2],\n",
    "                   [0.3, 0.3, 0.2, 0.2]\n",
    "                ]).unsqueeze(0) # for batch\n",
    "\n",
    "pred = torch.randn(16, 3, 13, 13, 4)\n",
    "gt = torch.randn(16, 3, 13, 13, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iouloss = IoULoss(method = 'CIoU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iouloss(pred, gt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.0625 / (0.25 + 0.16 - 0.0625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.atan(torch.tensor([1/6 * torch.pi]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 **(1/2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = {1, 5, 9, 13}\n",
    "# dilated convolution\n",
    "# kernel size 3: dilated ratio equals to k, max-pooling of stride 1, \n",
    "k = 5\n",
    "dilated_conv = torch.nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 1, dilation= k)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "inp = torch.randn(batch_size, 3, 256, 256)\n",
    "\n",
    "dilated_conv(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = nn.BatchNorm2d(32)\n",
    "samp = torch.randn((1, 32, 64, 64))\n",
    "\n",
    "bn(samp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = baseblock.conv[0]\n",
    "\n",
    "conv.bias.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseblock = BaseBlock(3, 64, 3, 1, 1)\n",
    "baseblock(torch.randn(1,3,24,24)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBlock(nn.Module) :\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0, act_fn = 'mish') :\n",
    "        super().__init__()\n",
    "        if act_fn.lower() == 'mish' :\n",
    "            activation = nn.Mish()\n",
    "        elif act_fn.lower() == 'leakyrelu' :\n",
    "            activation = nn.LeakyReLU()\n",
    "        elif act_fn.lower() == 'relu' :\n",
    "            activation = nn.ReLU()\n",
    "        else :\n",
    "            raise ValueError(f'{act_fn} activation function is not covered. add on DarkNetBottleneck module.')\n",
    "\n",
    "        self.activation = activation\n",
    "        self.conv = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size, stride = stride, padding = padding),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        self.activation)\n",
    "                        \n",
    "    def forward(self, x) :\n",
    "        return self.conv(x)\n",
    "        \n",
    "\n",
    "class DarkNetBottleneck(nn.Module) :\n",
    "    def __init__(self, in_channels, out_channels, expansion = 2, act_fn = 'mish') :\n",
    "        super().__init__()\n",
    "\n",
    "        mid_channels = int(out_channels / expansion)\n",
    "        ## Depthwise separable Block\n",
    "        self.conv1 = BaseBlock(in_channels, mid_channels, act_fn = act_fn, kernel_size = 1, stride = 1, padding = 0)\n",
    "        self.conv2 = BaseBlock(mid_channels, out_channels, act_fn = act_fn, kernel_size = 3, stride = 1, padding = 1)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        residual = x\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        output += residual\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSPStage(nn.Module) : \n",
    "    def __init__(self, in_channels, mid_channels, out_channels, block_fn, expansion, act_fn, num_blocks) :\n",
    "        '''\n",
    "        input x will be channel-wise splited into part1 and part2.\n",
    "        During CSP, only part2 will go through block_fn.\n",
    "        downsampling layer before CSP, and transition after concatenation.\n",
    "        input x : (B, C, H, W)\n",
    "        part1&part2 : (B, C // 2, H, W)\n",
    "        C should be divisible by 2.\n",
    "\n",
    "        in_channels : input channel of downsample layer. downsample layer reduce the feature size by 2\n",
    "        mid_channels : output channel of downsample layer and input channel of cspblock\n",
    "        block_fn : block function that will be applied on part2. For Darknet53, we are going to use DarkNetBottleneck.\n",
    "        expansion : expansion of block_fn. e.g) expansion=2, C_in -> C_out//2 -> C_out. C means channel.\n",
    "        act_fn : activation function. For DarkNet53, we are going to use mish.\n",
    "        num_blocks : number of iterations of block_fn\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.downsample = BaseBlock(in_channels, mid_channels, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.cspblock = nn.Sequential()\n",
    "\n",
    "        block_channels = mid_channels // 2 # input_channel for part2\n",
    "\n",
    "        for i in range(num_blocks) :\n",
    "\n",
    "            block = block_fn(in_channels = block_channels, \n",
    "                                             out_channels = block_channels,\n",
    "                                             expansion = expansion,\n",
    "                                             act_fn = act_fn\n",
    "                                             ) # this only covers DarkNetBottleneck module.\n",
    "\n",
    "            self.cspblock.add_module(f'partial_block_{i+1}', block )\n",
    "            \n",
    "        self.after_cspblock = BaseBlock(in_channels = block_channels, \n",
    "                                        out_channels = block_channels,\n",
    "                                        kernel_size = 1,\n",
    "                                        stride = 1,\n",
    "                                        padding = 0,\n",
    "                                        )\n",
    "        \n",
    "        self.transition = BaseBlock(in_channels = 2 * block_channels, \n",
    "                                        out_channels = out_channels,\n",
    "                                        kernel_size = 1,\n",
    "                                        stride = 1,\n",
    "                                        padding = 0,\n",
    "                                        )\n",
    "\n",
    "\n",
    "    def forward(self, x) :\n",
    "        x = self.downsample(x)\n",
    "        split = x.shape[1] // 2\n",
    "        part1, part2 = x[:, :split], x[:, split:]\n",
    "\n",
    "        part2 = self.cspblock(part2)\n",
    "        part2 = self.after_cspblock(part2).contiguous()\n",
    "\n",
    "        output = torch.cat([part1, part2], dim = 1)\n",
    "        output = self.transition(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = nn.ModuleList()\n",
    "\n",
    "modules.add_module('abc', nn.Conv2d(3,6,3,1,1,1))\n",
    "\n",
    "for m in modules :\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarkNet53(nn.Module) :\n",
    "    '''\n",
    "    initial layer : conv(3,3,32)/1, mish\n",
    "\n",
    "    in_channels  : [3, 32,  64,  64, 128,  256]\n",
    "    mid_channels : [64, 128, 256, 512, 1024]\n",
    "    out_channels : [64,  64, 128, 256,  512]\n",
    "\n",
    "    num_blocks of cspstage : [1,2,8,8,4]\n",
    "    '''\n",
    "    def __init__(self, act_fn, block_fn, expansion, in_channels_list = [], mid_channels_list = [], out_channels_list = [], num_blocks_list = []) :\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = BaseBlock(in_channels_list[0], in_channels_list[1], kernel_size = 3, stride = 1, padding = 1)\n",
    "        \n",
    "        self.modulelist = nn.Sequential()\n",
    "        for i, num_blocks in enumerate(num_blocks_list) :\n",
    "            \n",
    "            cspstage = CSPStage(in_channels = in_channels_list[i+1], \n",
    "                            mid_channels = mid_channels_list[i], \n",
    "                            out_channels = out_channels_list[i], \n",
    "                            block_fn = block_fn, \n",
    "                            expansion = expansion, \n",
    "                            act_fn = act_fn, num_blocks = num_blocks)\n",
    "            self.modulelist.add_module(f'CSPStage_{i+1}', cspstage)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        output = self.input_layer(x)\n",
    "        for stage in self.modulelist :\n",
    "            output = stage(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels_list  = [3, 32,  64,  64, 128,  256]\n",
    "mid_channels_list = [64, 128, 256, 512, 1024]\n",
    "out_channels_list = [64,  64, 128, 256,  512]\n",
    "num_blocks_list   = [1,2,8,8,4]\n",
    "\n",
    "\n",
    "model = DarkNet53(act_fn = 'mish', block_fn = DarkNetBottleneck, expansion = 2, \n",
    "                    in_channels_list = in_channels_list,\n",
    "                    mid_channels_list = mid_channels_list,\n",
    "                    out_channels_list = out_channels_list,\n",
    "                    num_blocks_list = num_blocks_list\n",
    "                    )\n",
    "\n",
    "model(torch.randn((1,3,512, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet BottleNeck\n",
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super().__init__()\n",
    "        inner_channels = 4 * growth_rate\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, inner_channels, 1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(inner_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(inner_channels, growth_rate, 3, stride=1, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.shortcut(x), self.residual(x)], 1)\n",
    "\n",
    "\n",
    "bottleneck = BottleNeck(in_channels = 3 , growth_rate= 12)\n",
    "\n",
    "bottleneck(torch.randn(1, 3, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Block: reduce feature map size and number of channels\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down_sample = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
    "            nn.AvgPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_sample(x)\n",
    "\n",
    "transition = Transition(3, 15)\n",
    "\n",
    "transition(torch.randn(1, 3, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet BottleNeck\n",
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super().__init__()\n",
    "        inner_channels = 4 * growth_rate\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, inner_channels, 1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(inner_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(inner_channels, growth_rate, 3, stride=1, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.shortcut(x), self.residual(x)], 1)\n",
    "\n",
    "# Transition Block: reduce feature map size and number of channels\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down_sample = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
    "            nn.AvgPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_sample(x)\n",
    "\n",
    "# DenseNet\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, nblocks, growth_rate=12, reduction=0.5, num_classes=10, init_weights=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.growth_rate = growth_rate\n",
    "        inner_channels = 2 * growth_rate # output channels of conv1 before entering Dense Block\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, inner_channels, 7, stride=2, padding=3),\n",
    "            nn.MaxPool2d(3, 2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "\n",
    "        for i in range(len(nblocks)-1):\n",
    "            self.features.add_module('dense_block_{}'.format(i), self._make_dense_block(nblocks[i], inner_channels))\n",
    "            inner_channels += growth_rate * nblocks[i]\n",
    "            out_channels = int(reduction * inner_channels)\n",
    "            self.features.add_module('transition_layer_{}'.format(i), Transition(inner_channels, out_channels))\n",
    "            inner_channels = out_channels \n",
    "        \n",
    "        self.features.add_module('dense_block_{}'.format(len(nblocks)-1), self._make_dense_block(nblocks[len(nblocks)-1], inner_channels))\n",
    "        inner_channels += growth_rate * nblocks[len(nblocks)-1]\n",
    "        self.features.add_module('bn', nn.BatchNorm2d(inner_channels))\n",
    "        self.features.add_module('relu', nn.ReLU())\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.linear = nn.Linear(inner_channels, num_classes)\n",
    "\n",
    "        # weight initialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(x.shape)\n",
    "        x = self.features(x)\n",
    "        print(x.shape)\n",
    "        x = self.avg_pool(x)\n",
    "        print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def _make_dense_block(self, nblock, inner_channels):\n",
    "        dense_block = nn.Sequential()\n",
    "        for i in range(nblock):\n",
    "            dense_block.add_module('bottle_neck_layer_{}'.format(i), BottleNeck(inner_channels, self.growth_rate))\n",
    "            inner_channels += self.growth_rate\n",
    "        return dense_block\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def DenseNet_121():\n",
    "    return DenseNet([6, 12, 24, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet_121()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.randn(1,3, 224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "samp = torch.randn((1, 512, 13,  13,))\n",
    "scales = [1, 5, 9, 13]\n",
    "\n",
    "for s in scales :\n",
    "    print(s, F.max_pool2d(samp, kernel_size = s, stride = 1, padding = s // 2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "class SPP(nn.Module) :\n",
    "    def __init__(self, scales: List = [1,5,9,13]) :\n",
    "        super().__init__()\n",
    "        '''\n",
    "        scales : list of kernel size for maxpooling. stride is fixed 1. padding is scale // 2 to fix the output shape.\n",
    "        output : input_channel x len(scales)\n",
    "        '''\n",
    "\n",
    "        self.pool = nn.Sequential()\n",
    "        for s in scales :\n",
    "            self.pool.add_module(f'maxpool_{s}x{s}', nn.MaxPool2d(kernel_size = s, stride = 1, padding = s // 2))\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        result = []\n",
    "        for pool in self.pool :\n",
    "            result.append(pool(x))\n",
    "\n",
    "        return torch.cat(result, dim = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp = SPP(scales = [1,5,9,13])\n",
    "\n",
    "spp(samp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FPN, PAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from YOLOv4.backbone import BaseBlock\n",
    "# from YOLOv4.neck import SPP\n",
    "\n",
    "class SPP(nn.Module) :\n",
    "    def __init__(self, scales = [1, 5, 9 , 13]) :\n",
    "        super().__init__()\n",
    "        '''\n",
    "        scales : list of kernel size for maxpooling. stride is fixed 1. padding is scale // 2 to fix the output shape.\n",
    "        output : input_channel x len(scales)\n",
    "        '''\n",
    "\n",
    "        self.pool = nn.Sequential()\n",
    "        for s in scales :\n",
    "            self.pool.add_module(f'maxpool_{s}x{s}', nn.MaxPool2d(kernel_size = s, stride = 1, padding = s // 2))\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        result = []\n",
    "        for pool in self.pool :\n",
    "            result.append(pool(x))\n",
    "\n",
    "        return torch.cat(result, dim = 1)\n",
    "\n",
    "x3_channel = 1024\n",
    "layer = nn.Sequential(*[BaseBlock(x3_channel, x3_channel, act_fn = 'leakyrelu') for _ in range(5)],\n",
    "                SPP(), # SPP expmand channels 4 times larger.\n",
    "                BaseBlock(x3_channel * 4, x3_channel)\n",
    "                                        )# 1024x19x19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 24, 24])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.randn(1, 1024, 24, 24)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BaseBlock(\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (1): BaseBlock(\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (2): BaseBlock(\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (3): BaseBlock(\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (4): BaseBlock(\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (5): SPP(\n",
       "    (pool): Sequential(\n",
       "      (maxpool_1x1): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (maxpool_5x5): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      (maxpool_9x9): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)\n",
       "      (maxpool_13x13): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (6): BaseBlock(\n",
       "    (activation): Mish()\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(4096, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Mish()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 448, 448])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample = nn.Upsample(scale_factor= 2, mode = 'bilinear')\n",
    "\n",
    "upsample(torch.randn((1,3, 224, 224))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "from YOLOv4.backbone import BaseBlock, DarkNetBottleneck, CSPResnet, DarkNet53\n",
    "\n",
    "\n",
    "class SPP(nn.Module) :\n",
    "    def __init__(self, scales: List = [1, 5, 9 , 13]) :\n",
    "        super().__init__()\n",
    "        '''\n",
    "        scales : list of kernel size for maxpooling. stride is fixed 1. padding is scale // 2 to fix the output shape.\n",
    "        output : input_channel x len(scales)\n",
    "        '''\n",
    "\n",
    "        self.pool = nn.Sequential()\n",
    "        for s in scales :\n",
    "            self.pool.add_module(f'maxpool_{s}x{s}', nn.MaxPool2d(kernel_size = s, stride = 1, padding = s // 2))\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        result = []\n",
    "        for pool in self.pool :\n",
    "            result.append(pool(x))\n",
    "\n",
    "        return torch.cat(result, dim = 1)\n",
    "\n",
    "class Yolov4(nn.Module) :\n",
    "    def __init__(self, in_channels_list, num_anchor, num_classes) :\n",
    "        super().__init__()\n",
    "        '''\n",
    "        backbone : list of 3\n",
    "        top-down :\n",
    "        bottom-up :\n",
    "        '''\n",
    "        final_channels = num_anchor * (5 + num_classes)\n",
    "        # backbone : [(cbl_3, SPP, clb_3), cbl, cbl] \n",
    "        # backbone output : [b1, b2, b3]\n",
    "        self.backbone = DarkNet53(act_fn = 'mish', block_fn = DarkNetBottleneck, expansion = 2, csp_func = CSPResnet,\n",
    "                                    in_channels_list = [3, 32,  32, 64, 128,  256, 512], \n",
    "                                    num_blocks_list = [1,2,8,8,4])\n",
    "        \n",
    "        # [256, 512, 1024]\n",
    "        x1_channel, x2_channel, x3_channel = in_channels_list\n",
    "\n",
    "\n",
    "        self.backbone_func1 = BaseBlock(in_channels = x1_channel, out_channels = x1_channel, kernel_size = 1, stride = 1, padding = 0, act_fn = 'leakyrelu' )# 256x76x76\n",
    "        self.backbone_func2 = BaseBlock(in_channels = x2_channel, out_channels = x2_channel, kernel_size = 1, stride = 1, padding = 0, act_fn = 'leakyrelu' )# 512x38x38\n",
    "        # CBL_3, SPP, CBL_3\n",
    "        self.backbone_func3 =  nn.Sequential(*[BaseBlock(x3_channel, x3_channel, act_fn = 'leakyrelu') for _ in range(5)],\n",
    "                                                SPP(), # SPP expand channels 4 times larger.\n",
    "                                                BaseBlock(x3_channel * 4, x3_channel)\n",
    "                                            )# 1024x19x19\n",
    "\n",
    "        self.path_func1 = nn.Sequential(*[BaseBlock(x3_channel, x3_channel, act_fn = 'leakyrelu') for _ in range(5)],\n",
    "                                          nn.Upsample(scale_factor= 2, mode = 'bilinear'),\n",
    "                                            BaseBlock(x3_channel, x3_channel // 2)\n",
    "                                            )# CBL_5, UP, CBL\n",
    "                        \n",
    "        # CBL_5, UP, CBL\n",
    "        self.path_func2 = nn.Sequential(*[BaseBlock(x2_channel, x2_channel, act_fn = 'leakyrelu') for _ in range(5)],\n",
    "                                          nn.Upsample(scale_factor= 2, mode = 'bilinear'),\n",
    "                                            BaseBlock(x2_channel, x2_channel // 2)\n",
    "                                            )# CBL_5, UP, CBL\n",
    "        \n",
    "        self.head_func1 = BaseBlock(in_channels = x1_channel, out_channels = x1_channel * 2, kernel_size = 1, stride = 2, padding = 0, act_fn = 'leakyrelu' )# CBL\n",
    "        self.head_func2_1 = nn.Sequential(*[BaseBlock(x2_channel, x2_channel, act_fn = 'leakyrelu') for _ in range(5)])# CBL_5\n",
    "        self.head_func2_2 = BaseBlock(in_channels = x2_channel, out_channels = x2_channel * 2, kernel_size = 1, stride = 2, padding = 0, act_fn = 'leakyrelu' )# CBL# CBL\n",
    "        self.head_func3 = nn.Sequential(*[BaseBlock(x3_channel, x3_channel, act_fn = 'leakyrelu') for _ in range(5)])# CBL_5\n",
    "\n",
    "        self.result_func1 = BaseBlock(in_channels = x1_channel, out_channels = final_channels, kernel_size = 1, stride = 1, padding = 0, act_fn = 'leakyrelu' )# CBL# CBL\n",
    "        self.result_func2 = BaseBlock(in_channels = x2_channel, out_channels = final_channels, kernel_size = 1, stride = 1, padding = 0, act_fn = 'leakyrelu' )# CBL# CBL\n",
    "        self.result_func3 = BaseBlock(in_channels = x3_channel, out_channels = final_channels, kernel_size = 1, stride = 1, padding = 0, act_fn = 'leakyrelu' )# CBL# CBL\n",
    "\n",
    "\n",
    "    # top-down : [(cbl_5, up, cbl), (cbl_5, up, cbl)]\n",
    "    def top_down(self, b1, b2, b3, func1, func2) :\n",
    "        '''\n",
    "        b represents backbone output.\n",
    "        b1 has the largest feature size and b3 has the smallest.\n",
    "        func{num} corresponds to b{num}. \n",
    "        '''\n",
    "        \n",
    "        p3 = b3\n",
    "        p2 = func1(p3) + b2\n",
    "        p1 = func2(p2) + b1\n",
    "\n",
    "        return (p1, p2, p3)\n",
    "\n",
    "    def bottom_up(self, x1, x2, x3, func1, func2_1, func2_2, func3) :\n",
    "        '''\n",
    "        x1 has the largest feature size and x3 has the smallest.\n",
    "        func{num} corresponds to x{num}. \n",
    "        func2 is splitted to func2_1 and func2_2 as the diagram\n",
    "        '''\n",
    "        r1 = x1\n",
    "        r2 = func2_1((func1(x1) + x2))\n",
    "        r3 = func3(func2_2(r2) + x3)\n",
    "\n",
    "        return (r1, r2, r3)\n",
    "\n",
    "\n",
    "    def forward(self, x) :\n",
    "        b1, b2, b3 = self.backbone(x)\n",
    "        b1 = self.backbone_func1(b1)\n",
    "        b2 = self.backbone_func2(b2)\n",
    "        b3 = self.backbone_func3(b3)\n",
    "\n",
    "        # top down\n",
    "        p1, p2, p3 = self.top_down(b1, b2, b3, self.path_func1, self.path_func2)\n",
    "\n",
    "        # bottom up\n",
    "        r1, r2, r3 = self.bottom_up(p1, p2, p3, self.head_func1, self.head_func2_1, self.head_func2_2, self.head_func3)\n",
    "\n",
    "        output1 = self.result_func1(r1)\n",
    "        output2 = self.result_func2(r2)\n",
    "        output3 = self.result_func3(r3)\n",
    "\n",
    "        return (output1, output2, output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PANet(in_channels_list=[256, 512, 1024], num_anchor = 3, num_classes = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 76, 76]) torch.Size([1, 512, 38, 38]) torch.Size([1, 1024, 19, 19])\n",
      "torch.Size([1, 256, 76, 76]) torch.Size([1, 512, 38, 38]) torch.Size([1, 1024, 19, 19])\n",
      "torch.Size([1, 256, 76, 76]) torch.Size([1, 512, 38, 38]) torch.Size([1, 1024, 19, 19])\n"
     ]
    }
   ],
   "source": [
    "samp = torch.randn(1, 3, 608, 608)\n",
    "\n",
    "r1, r2, r3 = model(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 54, 76, 76])\n",
      "torch.Size([1, 54, 38, 38])\n",
      "torch.Size([1, 54, 19, 19])\n"
     ]
    }
   ],
   "source": [
    "print(r1.shape)\n",
    "print(r2.shape)\n",
    "print(r3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x4\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x4' is not defined"
     ]
    }
   ],
   "source": [
    "# SAM (spatial attention module)\n",
    "# maxpooling, average pooling, concat along channels, convolution, sigmoid, and multiply\n",
    "# modified version : convolution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "dropblock = torchvision.ops.DropBlock2d(p = 1, block_size = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "result = dropblock(torch.ones((1,1,24, 24)))\n",
    "plt.imshow(result[0][0].detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "dropblock.eval()\n",
    "result = dropblock(torch.ones((1,1,24, 24)))\n",
    "plt.imshow(result[0][0].detach().cpu().numpy()*255.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
