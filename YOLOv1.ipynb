{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3295619247.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    for step, data in enumerate(dataloader) :\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "## train\n",
    "\n",
    "\n",
    "for step, data in enumerate(dataloader) :\n",
    "\n",
    "    data\n",
    "\n",
    "    pred = model(x) \n",
    "    loss = criterion(pred, gt)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name', 'attributes', 'timestamp', 'labels', 'imgsize'])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11ecaf4a-837e3550.jpg\n",
      "272cd572-f7289b9d.jpg\n",
      "282678b0-5f4e4eb3.jpg\n",
      "31a83844-ba334636.jpg\n",
      "321877a3-f277463d.jpg\n",
      "48f20d4e-504d2377.jpg\n",
      "49cf8611-8991f7a7.jpg\n",
      "51a2ee54-e7f7d10f.jpg\n",
      "57ea20aa-d836f65b.jpg\n",
      "65c115f0-324deb97.jpg\n"
     ]
    }
   ],
   "source": [
    "categories = []\n",
    "\n",
    "with open('data/label/det_train.json') as f :\n",
    "    json_info = json.load(f)\n",
    "\n",
    "\n",
    "for info in json_info :\n",
    "    try :\n",
    "        labels = info['labels']\n",
    "    except :\n",
    "        print(info['name'])\n",
    "        continue\n",
    "    for l in labels :\n",
    "        categories.append(l['category'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "train_imgfiles = glob('data/images/train/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02b7b9ee-74fb981c.jpg data/images/train\\02b7b9ee-74fb981c.jpg\n"
     ]
    }
   ],
   "source": [
    "print(json_info[1000]['name'], train_imgfiles[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDDDataset(Dataset) :\n",
    "    def __init__(self, imgdir, jsonfile, transform = None ) :\n",
    "        super().__init__()\n",
    "        self.imgdir = imgdir\n",
    "        self.imgfiles = glob(os.path.join(self.imgdir, '*.jpg'))\n",
    "        \n",
    "        with open(jsonfile, 'r') as f :\n",
    "            json_infos = json.load(f)\n",
    "        self.json_infos = json_infos\n",
    "\n",
    "        ## make sure json information and imgfile are matched\n",
    "        for info, imgfile in tqdm(zip(self.json_infos, self.imgfiles), desc = 'validate json and image matching...') :\n",
    "            os.path.basename(imgfile) == info['name']\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        imgfile = self.imgfiles[idx]\n",
    "        json_info = self.json_infos[idx]\n",
    "\n",
    "        json_info = json_info['labels'] if 'labels' in json_info.keys() else []\n",
    "\n",
    "        image = plt.imread(imgfile)\n",
    "        if self.transform :\n",
    "            image = self.transform(image)\n",
    "\n",
    "        ## bbox handling\n",
    "        for info in json_info :\n",
    "            label = info['category']\n",
    "            coord = info['box2d']\n",
    "            x, y = coord['x'], coord['y']\n",
    "            w, h = coord['w'], coord['h']\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.imgfiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO(nn.Module):\n",
    "    def __init__(self, VGG16, num_grids = 7, numBox = 2, num_classes = 13):\n",
    "        '''\n",
    "        VGG16 : backbone network\n",
    "        num_grids : # of grids to divde an image, make sure to be dividable with image size\n",
    "        numBox : number of boxes per grid\n",
    "        num_classes : # of classes. In BDD, there are 13 classes\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = VGG16\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 512,out_channels = 1024, kernel_size = 3, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels = 1024,out_channels = 1024, kernel_size = 3, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels = 1024,out_channels = 1024, kernel_size = 3, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels = 1024,out_channels = 1024, kernel_size = 3, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear((num_grids**2) * 1024, 4096),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, (num_grids**2)*(numBox ))\n",
    "        )\n",
    "\n",
    "        # 가중치 초기화\n",
    "        for m in self.conv.modules():\n",
    "    \t    if isinstance(m, nn.Conv2d) :\n",
    "\t\t        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "                \n",
    "        for m in self.linear.modules():\n",
    "        \tif isinstance(m, nn.Linear) :\n",
    "                 nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "                \n",
    "\n",
    "    # 정전파 \n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = self.conv(out)\n",
    "        out = self.linear(out)\n",
    "        out = torch.reshape(out, (-1 ,7, 7, 30))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1470 / 7 / 7 / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
