{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 ways to improve the performance\n",
    "# yolov2 : changes in model configurations\n",
    "# yolo9000 : changes in model training method. joint training of classification and object detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "\n",
    "from global_utils import IoU\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-mean clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://towardsdatascience.com/create-your-own-k-means-clustering-algorithm-in-python-d7d4c9077670\n",
    "\n",
    "def KmeanClustering(bboxes, max_iters, k = 5, plot = True) :\n",
    "    '''\n",
    "    read out whole bounding boxes from train datasets, and perform k mean clusterings\n",
    "    bboxes = (x_center, y_center, w, h). Here ratios are respect to the image size, NOT grid cell.\n",
    "    max_iters : number of maximum iterations\n",
    "    k : number of clusters\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # initialize the centroid\n",
    "    min_w, min_h = np.min(bboxes[:,2]), np.min(bboxes[:,3])\n",
    "    max_w, max_h = np.max(bboxes[:,2]), np.max(bboxes[:,3])\n",
    "    # uniformly distribute the k cluster centroids\n",
    "    centroids = np.array([[np.random.uniform(min_w, max_w), np.random.uniform(min_h, max_h)] for i in range(k)])\n",
    "    \n",
    "    prev_centroids = None\n",
    "    iteration = 0\n",
    "\n",
    "    print('initial centroids : ', centroids)\n",
    "    if plot :\n",
    "        plt.scatter(bboxes[:,2], bboxes[:,3])\n",
    "        plt.title('original points')\n",
    "        plt.show()\n",
    "\n",
    "    while np.not_equal(centroids, prev_centroids).any() or iteration < max_iters :\n",
    "        \n",
    "        sorted_points = [[] for _ in range(k)]\n",
    "\n",
    "        for (_, _, w, h) in bboxes :\n",
    "            \n",
    "            # Suppose x_center and y_center are same for centroid and data point here, we just want the ratio of w, h.\n",
    "            x, y = 0.5, 0.5\n",
    "\n",
    "            dist = np.array([IoU(torch.tensor([x, y, w, h]), \n",
    "                                 torch.tensor([x, y, centroid_w, centroid_h ])) \n",
    "                                 for centroid_w, centroid_h in centroids]) # IoU supports only torch for now\n",
    "            centroid_idx = np.argmax(dist)\n",
    "            sorted_points[centroid_idx].append([w, h])\n",
    "\n",
    "        prev_centroids = centroids\n",
    "        centroids = [[np.mean([c[0] for c in cluster]), np.mean([c[1] for c in cluster])] for cluster in sorted_points ]\n",
    "\n",
    "    if plot :\n",
    "        color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                    for i in range(k)]\n",
    "\n",
    "        for i, cluster in enumerate(sorted_points) :\n",
    "            if len(cluster) :\n",
    "                plt.scatter(np.array(cluster)[:,0], np.array(cluster)[:, 1], color = color[i])\n",
    "        plt.title('after clustering')\n",
    "        plt.show()\n",
    "\n",
    "        # if any new centroids has NaN, then substitute back to previous centroids\n",
    "        # centroids[np.isnan(centroids)] = prev_centroids[np.isnan(centroids)]\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return centroids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_info(jsonfile) :\n",
    "    with open(jsonfile, 'r') as f :\n",
    "        json_infos = json.load(f)\n",
    "\n",
    "    total_bboxes = []\n",
    "    for js_info in tqdm(json_infos) :\n",
    "        img_h, img_w = js_info['imgsize']\n",
    "        label_info = js_info['labels'] if 'labels' in js_info.keys() else []\n",
    "        for idx in range(len(label_info)) :\n",
    "            label = label_info[idx]['category']\n",
    "            coord = label_info[idx]['box2d']\n",
    "\n",
    "            x,y,w,h = coord['x'], coord['y'], coord['w'], coord['h']\n",
    "            total_bboxes.append([x, y, w, h])\n",
    "\n",
    "    return total_bboxes\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 54097.64it/s]\n"
     ]
    }
   ],
   "source": [
    "## Test Case\n",
    "\n",
    "bboxes = read_json_info(jsonfile = 'data/label/yolo_det_val.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial centroids :  [[0.11339715 0.02097848]\n",
      " [0.34259652 0.19360561]\n",
      " [0.29019374 0.43973265]\n",
      " [0.35291663 0.87103131]\n",
      " [0.32545784 0.60087609]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g3/glr70r5n6z5gjn30xdr24fhc0000gn/T/ipykernel_4465/3694228162.py:36: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  dist = np.array([IoU(torch.tensor([x, y, w, h]),\n",
      "/var/folders/g3/glr70r5n6z5gjn30xdr24fhc0000gn/T/ipykernel_4465/3694228162.py:36: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dist = np.array([IoU(torch.tensor([x, y, w, h]),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g3/glr70r5n6z5gjn30xdr24fhc0000gn/T/ipykernel_4465/1558913695.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKmeanClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/g3/glr70r5n6z5gjn30xdr24fhc0000gn/T/ipykernel_4465/3694228162.py\u001b[0m in \u001b[0;36mKmeanClustering\u001b[0;34m(bboxes, max_iters, k, plot)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             dist = np.array([IoU(torch.tensor([x, y, w, h]), \n\u001b[0m\u001b[1;32m     37\u001b[0m                                  torch.tensor([x, y, centroid_w, centroid_h ])) \n\u001b[1;32m     38\u001b[0m                                  for centroid_w, centroid_h in centroids]) # IoU supports only torch for now\n",
      "\u001b[0;32m/var/folders/g3/glr70r5n6z5gjn30xdr24fhc0000gn/T/ipykernel_4465/3694228162.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             dist = np.array([IoU(torch.tensor([x, y, w, h]), \n\u001b[0m\u001b[1;32m     37\u001b[0m                                  torch.tensor([x, y, centroid_w, centroid_h ])) \n\u001b[1;32m     38\u001b[0m                                  for centroid_w, centroid_h in centroids]) # IoU supports only torch for now\n",
      "\u001b[0;32m~/Documents/OBJECT_DETECTION/global_utils.py\u001b[0m in \u001b[0;36mIoU\u001b[0;34m(pred, gt)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mintersection\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_area\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mintersection\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "centroids = KmeanClustering(np.array(bboxes)[:500], max_iters = 10, k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[[1,2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXd0lEQVR4nO3df5RdZX3v8c93733mNyHESQhN6IQmkIhEkzi9oFFqhYVc+RFsquJd+IOKXLVY9Xpvl3J7F7Bu1f7hsnbVWzVNi7ayoEqoILKAanWh3DbLyQ8bIAQSy5BQQiZCSDLJzDl77+/9YyZcTGaSc86cfc4+57xfa7GYzNnPOZ+zAp88eeY5zzZ3FwAgv4JGBwAAnBxFDQA5R1EDQM5R1ACQcxQ1AORclMWT9vf3+6JFi7J4agBoSZs2bdrv7nOneiyTol60aJGGhoayeGoAaElmNjzdYyx9AEANpGms8fGDStOk5s+dyYwaANpBHI9r544faGjj1/Sr/U8pCCKlaazX9J+nwQs/piVLr1AUdc74dSyLTyYODg46Sx8AWtne57fq3u9+QElaUqk4esLjhUKvwrCgNe/+e80/6w2nfD4z2+Tug1M9xtIHAFRo7/O/0Ia73quxsQNTlrQklUqjGhs7oA13vUd7n//FjF6PogaACsTxuO797vsVl46Wd33p6MT18XjVr0lRA0AFdu74gZKkVNGYJClp544Hqn5NihoAKjC08WsqlaZe7phOqTSqoY1/VfVrUtQAUKY0TfSr/U9VNfZX+5+qeuseRQ0AZSqVRhUE1e1qDoKo4pn4K2OrGgUAbahQ6FWaxlWNTdNYhUJvVWMpagAoUxCEek3/eVWNfU3/eQqCsLrXrWoUALSpwQs/VvHMuFDo1eCFH6/6NSlqAKjAkqVXKAwLFY0Jw4KWLH1n1a9JUQNABaKoU2ve/feKCt3lXV/onrh+Bmd+UNQAUKH5Z71Ba6/9jrq6Zk+7DFIo9Kqra7bWXvudss76OBlOzwOAKsw/6w368Md/rp07HtDQxr+a4vS8j2vJ0nfW5PQ8ihoAqhRFnVr2undp2evepTRNVCqNqlDorXp3x7SvU9NnA4A2FQShOjtnZfPcmTwrAKBmKGoAyDmKGgByjqIGgJyjqDVxdGExPqrU00ZHAYATtO2ujySNtWffFm0f/pEOju5VYIFSTzWrd75eO3CJFs5bqbDK4wwBoJbasol+dXBYP936daWeKE4m7mOW+sSB3gdHn9emHd/Vlqfu0cUrPqo5swYaGRUA2m/p48WDw/rJ5q+qGB95paSPFyfjKsZH9OPNX9WLB4frnBAAfl1bFXWSxnpk69eVpMUyry9OXl/dQeEAUAtlFbWZfdrMHjezx8zsTjPryjpYFvbs2/LKEke5Uk+0Z9/WbAIBQBlOWdRmtkDSH0kadPcLJIWSrs06WBa2D/9o2uWO6cTJuLYP/zCjRABwauUufUSSus0sktQj6T+yi5SN1FMdHN1b1diDo3vZugegYU5Z1O7+nKQvSXpW0vOSXnb3h4+/zsxuNLMhMxsaGRmpfdIZipNxBVbdkrxZUPFMHABqpZyljzMkrZF0jqTfkNRrZtcdf527r3P3QXcfnDt3bu2TzlAUdlY9K3ZPFYUzP1MWAKpRzhTzUkn/7u4j7l6SdI+kN2cbq/YCCzSrd35VY2f1zq96Ng4AM1VO+zwr6SIz6zEzk3SJpO3ZxsrGawcuqXhmHIWdeu3ApRklAoBTK2eNeqOkuyVtlrRtcsy6jHNlYuG8lQqssjsvBBZq4bwV2QQCgDKU9fd5d7/F3Ze5+wXu/n53b8qfrIVBpItXfFRh0FHm9R2T17flJ+0B5ETbLbzOmTWg3111kzqinmmXQaKwUx1Rj3531U2c9QGg4dpyqjhn1oCuesv/1p59W7V9+Ic6OLpXZoH8ldPzLtXCeSuYSQPIhbZtojCINDB/UAPzB5V6qjgZVxR2srsDQO60bVG/WmCBOqLuRscAgCkxfQSAnKOoASDnKGoAyDmKGgByjqIGWljisUb9ZSUV3jAD+cKuD6DFlHxcj6bf0z3JV7RbTypUpESxztYy/V74Ka0OrlHBOA2ymTCjBlrIU+kmXV9apq8nn9Gz2i6XK1ZJLtez2q6vJ/9N15eW6el0c6OjogIUNdAink4363/FV+mwXtKYDk95zZhGdVgv6U/iKynrJkJRAy2g5OO6LV6rcR0p6/pxHdFt8VqVmvN8tbZDUQMt4NH0e4pVrGhMrKL+b3pvRolQSxQ10ALuSb6iMY1WNGZMo9qQ/HlGiVBLFDXQ5BJPtFtPVjV2t55k614ToKiBJjemwwqr3GkbKJr2B4/ID4oaaHJd6lOiuKqxqWJ1qa/GiVBrFDXQ5EILdbaWVTX2bC1TWOF9RFF/FDXQAn4v/JS61FvRmC71aW346YwSoZYoaqAFrA6uUaTybtp8TKSC3hysySgRaomiBlpAwTp1S7RBneop6/pO9eiWaANnfjQJihpoEecGq/Sn0f3q0xnTLoN0qU99OkN/Gt2vc4NVdU6IalHUQAs5N1il2wtP6mPhn+s39VqZTKEKMpl+U+frY+GXdXvhSUq6yXDMKdBiCtap3wnfo98J36PEE43psLrUx+6OJkZRAy0stFC9Or3RMTBDLH0AQM5R1ACQcxQ1AOQcRQ0AOUdRA0DOUdQAkHMUNQDkHEUNADlHUQNAzpVV1GY228zuNrMnzWy7mb0p62AAgAnlfoT8LyQ96O6/b2YdUplnKQIAZuyURW1mp0u6WNKHJMndi5KK2cYCABxTztLHOZJGJN1uZlvMbL2ZnXDYrZndaGZDZjY0MjJS86AA0K7KKepI0ipJX3P3lZJGJX32+IvcfZ27D7r74Ny5c2scEwDaVzlFvUfSHnffOPnruzVR3ACAOjhlUbv7Xkm7zWzp5LcukfREpqkAAK8od9fHJyTdMbnj45eSrs8uEgDg1coqanffKmkw2ygAgKnwyUQAyDmKGgByjqIGgJyjqAEg5yhqAMg5ihoAco6iBoCco6gBIOdyVdRpnKp4qKQ08UZHAYDcKPcj5JlJiomGH9qrx9bv0oFdhxVEpjR2zV7Spws+vFgD75ivsCNsdEwAaBhzr/3sdXBw0IeGhk553f5/O6AffvTnSkup4iPJCY9HPaGCQqBLv/Hb6l8+u+Y5ASAvzGyTu095VEfDlj72bzugh/9go4ovl6YsaUmKjyQqvlzSw9dv1P5tB+obEAByoiFFnRQT/fC//lzx0akL+njx0Ynrk2J51wNAK2lIUQ8/tFdpKa1oTFpKNfzw3owSAUB+NaSoH1u/a9rljunERxI9tn5XRokAIL/qXtRp4jqw63BVYw/sPMzWPQBtp+5FHR+JFURW1dggNMVH4honAoB8q3tRRz2R0ri6WXGauKKehm/9BoC6qntRB6Fp9uK+qsbOXtKnIKxuNg4AzaohP0y84IbFinoq+7Rh1BPqghsWZ5QIAPKrIUU98I75CgqVvXRQCDRw2fyMEgFAfjWkqMOOUJd+47cVdZc3q466J67nzA8A7ahhHyHvXz5bl91+oTpOL0y7DBL1hOo4vaDLbr+Qsz4AtK2GbqHoXz5b7/7J2zX88OTpeTsPKwhNaTJ5et4NizVwGafnAWhvDd/rFnaE+q0rF+i3rlygNHHFR2JFPRG7OwBgUsOL+tWC0NRxWqHRMQAgV3J1hxcAwIkoagDIOYoaAHKOogaAnKOoAbSUOE10aPyIkrSym5PkWa52fQBANYpJSQ/u+hf99ZZ7tfOlPYqCUHGaaMmchfrIijW6fPGb1BE2746yht6FHABm6t9e2KmPPPAFxWms0dLYCY/3FLpUCCKtv+JmLZ+3pAEJy5PLu5ADwExt27dTH/z+bXp5/PCUJS1JR0pjenn8sD5w323atm9nnRPWBkUNoCkVk5Ju+MEXdDQeL+v6o/G4bvjBF1RMShknq72yi9rMQjPbYmb3ZxkIAMrx4K5/USmt7NZ8pTTWQ7v+NaNE2alkRv1JSduzCgIAlfjrLffqyDTLHdM5UhrTuq3fyyZQhsoqajNbKOkKSeuzjQMAp5akqXa+tKeqsTtf3NN0W/fKnVF/RdIfS5r23ZnZjWY2ZGZDIyMjtcgGAFM6UhpTFFR3/HEYBBXPxBvtlEVtZldK2ufum052nbuvc/dBdx+cO3duzQICwPF6Cl2K06SqsUmaqqfQVeNE2SpnRr1a0tVm9oykuyS93cy+nWkqADiJMAi05IyFVY1dMmehwqC5NrydMq27f87dF7r7IknXSvpnd78u82QAcBIfWbmm4plxb6FLN664JptAGWquP1YAYNLli9+kQlDZKRhREOkdiy/KKFF2Kipqd/+Ju1+ZVRgAKFdHWND6K25Wd9RZ1vXdUafWX3FzU575wYwaQNNaPm+J/u7qW3R6Z9+0yyC9hS6d3tmnv7v6llyf9XEynJ4HoKktn7dEP/3AN/TQrn/Vuq3f084X9ygMAiVpqiVzFurGFdfoHYsvasqZ9DEUNYCm1xEWdNV5b9VV571VSZrqSGlMPYWuptvdMR2KGkBLCYNAp3X2NDpGTbXGHzcA0MIoagDIOYoaAHKOogaAnKOoASDnKGoAyDmKGgByjqIGgJyjqAEg5yhqAMg5ihoAco6iBoCco6gBIOcoagDIOYoaAHKOogaAnKOoASDnKGoAyDmKGgByjqIGgJyjqAEg5yhqAMg5ihoAco6iBoCco6gBIOcoagDIOYoabSNNEhVHR5UmSaOjABWJGh0AyFJSLOmZRx7Vtn/YoAPDuxWEodIk0eyBs7X8vWu16OLVCjsKjY4JnJS5e82fdHBw0IeGhmr+vEAlRp58Sv90821K40Tx0aMnPB51dymIIl32xVvVv/TcBiQE/j8z2+Tug1M9xtIHWtL+HU/rof/xJyoeOjxlSUtSfHRMxUOH9eB//5/av+PpOicEykdRo+UkxZIe/tytisfGy7o+HhvXw5+7VUmxlHEyoDqnLGozO9vMfmxmT5jZ42b2yXoEA6r1zCOPKo3jisakcaxnfvpoRomAmSlnRh1L+oy7ny/pIkl/aGbnZxsLqN62f9ig+OhYRWPio2PadteGjBIBM3PKonb359198+TXhyRtl7Qg62BANdIk0YHh3VWNPTC8m617yKWK1qjNbJGklZI2TvHYjWY2ZGZDIyMjNYoHVCYeG1MQhlWNDcJA8VhlM3GgHsouajPrk7RB0qfc/eDxj7v7OncfdPfBuXPn1jIjULaoq6vqWXGapIq6umqcCJi5sorazAqaKOk73P2ebCMB1QvCULMHzq5q7OyBs6uejQNZKmfXh0n6G0nb3f3L2UcCZmb5e9cq6q5sZhx1d2n5tWszSgTMTDkz6tWS3i/p7Wa2dfKfd2acC6jaootXK4gqOx0hiCIteuvqjBIBM1POro+fubu5++vdfcXkPw/UIxxQjbCjoMu+eKuirs6yro+6OnXZF2/lzA/kFp9MREvqX3quLv/S59VxWt+0yyBRd5c6TuvT5V/6PGd9INc4PQ8tq3/puXrvXd/UMz99VNvuOnZ6XqA0SSdOz7t2rRa9ldPzkH8UNVpa2FHQ4kvepsWXvE1pkigeG1PU1cXuDjQVihptIwhDdfT2NjoGUDHWqAEg5yhqAMg5ihoAco6iBoCco6gbLPZEB9MxJZ42OgqAnGLXRwOMe6zvH31cXz30qHbE+xQpVKxES6N5uum01bqq+3XqNH5rAEzgLuR1trm4R/9l/x0qKdGoF094vNc6VFCoO/uv08oO7s8AtAvuQp4TW4rP6ff3f0sH/OiUJS1Jo17UAT+qtfu/qS3F5+qcEEAeUdR1Mu6x3rf/2zri5d3p+oiX9L7939a4V3aTVgCth6Kuk+8ffVwlVXbnkZIS3X/0iYwSAWgWFHWdfPXQo9Mud0xn1Iv6y0M/yygRgGZBUddB4ql2xPuqGrsj3sfWPaDNUdR1MOpFRarutLZIQcUzcQCthaKug17rUFzh+vQxsVL1WkeNEwFoJhR1HYQWaGk0r6qxS6N5Co3fJqCd0QB1ctNpqyueGfdahz5x2lsySgSgWVDUdXJV9+tUqHCduqBQV3afn1EiAM2Coq6TTot0Z/916rHy7s/XYwXd2X8dZ34AoKjraWXHAm3o/5BmW/e0yyC91qHZ1q0N/R/irA8Akjg9r+5WdizQL876jO4/+oT+8tDPJk/PCxQr1dJonj5x2lt0Zff5zKQBvII2aIBOi7S25/Va2/N6JZ5q1IvqtQ52dwCYEkXdYKEFmmVdjY4BIMeYwgFAzlHUAJBzFDUA5BxFDQA5R1EDaClp6orHE3la+/vBNgq7PgA0vTRJtW/nYQ1vekmjLxZlgeSp1DunQwNvPEPzlvQpCJt3XkpRA2hqB18Y09b7npOnrqQ0MYs+dq+N0ReL2vGTfXrqkRGtuHqBZp3ZnFthm/ePGABt7+ALY9r8j3sUj6evlPTxkpIrHk+1+R/36OALY3VOWBsUNYCmlCaptt73nNK4vLXoNPaJ65Pmu7VdWUVtZpeb2Q4z22lmn806FACcyr6dhyv+gaGnrn07D2eUKDunLGozCyX9H0n/WdL5kt5nZhySDKChhje9NO1yx3SSkmt400sZJcpOOTPq/yRpp7v/0t2Lku6StCbbWAAwPU9doy9Wd9Pn0ReLTbd1r5yiXiBp96t+vWfye7/GzG40syEzGxoZGalVPgA4QVJKVe1hkxZMjG8mNfthoruvc/dBdx+cO3durZ4WAE4QFoJXtuBVytOJ8c2knLTPSTr7Vb9eOPk9AGgIC0y9cyq7WfQxvXM6ZIHVOFG2yinqn0s618zOMbMOSddKui/bWABwcgNvPENhobLCDQumgTeekVGi7JyyqN09lnSTpIckbZf0HXd/POtgAHAy85b0VTwztsA0b0lfRomyU9ZCjbs/4O7nuftid/981qEA4FSCMNCKqxcoiMor6yCyieub8MyP5ksMAJNmndmlVe9aqKgzmHYZJCyYos5Aq961sGnP+uBQJgBNbdaZXXrLH5zD6XkAkGdBGGj+0lmav3TW5Cl6qcJC0HS7O6ZDUQNoKRaYos6w0TFqytxr/1FKMxuRNFzl8H5J+2sYpxnwnltfu71fifdcqQF3n/LTgpkU9UyY2ZC7DzY6Rz3xnltfu71fifdcS827ug4AbYKiBoCcy2NRr2t0gAbgPbe+dnu/Eu+5ZnK3Rg0A+HV5nFEDAF6FogaAnMtNUbfbDXTN7Gwz+7GZPWFmj5vZJxudqV7MLDSzLWZ2f6Oz1IOZzTazu83sSTPbbmZvanSmrJnZpyf/u37MzO40s+Y8ZOMkzOxvzWyfmT32qu/NMbN/MrOnJ/9dkzNVc1HUbXoD3VjSZ9z9fEkXSfrDNnjPx3xSE0fmtou/kPSguy+T9Aa1+Hs3swWS/kjSoLtfICnUxDn2reabki4/7nuflfQjdz9X0o8mfz1juShqteENdN39eXffPPn1IU38z3vCvShbjZktlHSFpPWNzlIPZna6pIsl/Y0kuXvR3Q80NFR9RJK6zSyS1CPpPxqcp+bc/RFJLx737TWSvjX59bckXVOL18pLUZd1A91WZWaLJK2UtLHBUerhK5L+WFJz3V20eudIGpF0++Ryz3oz6210qCy5+3OSviTpWUnPS3rZ3R9ubKq6OdPdn5/8eq+kM2vxpHkp6rZlZn2SNkj6lLsfbHSeLJnZlZL2ufumRmepo0jSKklfc/eVkkZVo78O59XkuuwaTfwh9RuSes3susamqj+f2Ptck/3PeSnqtryBrpkVNFHSd7j7PY3OUwerJV1tZs9oYnnr7Wb27cZGytweSXvc/djflu7WRHG3sksl/bu7j7h7SdI9kt7c4Ez18oKZnSVJk//eV4snzUtRt90NdM3MNLFuud3dv9zoPPXg7p9z94XuvkgTv8f/7O4tPdNy972SdpvZ0slvXSLpiQZGqodnJV1kZj2T/51fohb/Aeqr3Cfpg5Nff1DSvbV40lycR+3usZkdu4FuKOlv2+AGuqslvV/SNjPbOvm9m939gcZFQkY+IemOyUnILyVd3+A8mXL3jWZ2t6TNmtjdtEUt+HFyM7tT0tsk9ZvZHkm3SPozSd8xsw9r4qjn99TktfgIOQDkW16WPgAA06CoASDnKGoAyDmKGgByjqIGgJyjqAEg5yhqAMi5/wfq2xsEhzBaSAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## visualize ratio box\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "number_of_colors = 8\n",
    "\n",
    "color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "             for i in range(number_of_colors)]\n",
    "\n",
    "\n",
    "for i in range(number_of_colors):\n",
    "    plt.scatter(random.randint(0, 10), random.randint(0,10), c=color[i], s=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "reference : https://github.com/rafaelpadilla/Object-Detection-Metrics\n",
    "usage : validation, evaluation. \n",
    "predictions and gts will be handed by reading gtfile directly, since dataloader limits the number of gt bounding box\n",
    "'''\n",
    "\n",
    "\n",
    "from global_utils import IoU\n",
    "\n",
    "## MAP\n",
    "\n",
    "class MeanAveragePrecisionMetrics :\n",
    "    def __init__(self, gts, preds, iou_threshold_range, confidence_threshold) :\n",
    "        '''\n",
    "        gts, preds = [[[class, x, y, w, h, c],...], ...] # imgs x bboxes\n",
    "\n",
    "        classes : should be collected from gt.\n",
    "        iou_threshold_range : (min_threshold, interval, max_threshold). e.g) IoU(0.6, 0.1, 0.9) = [0.6, 0.7, 0.8, 0.9]\n",
    "        confidence_threshold : predicted bounding boxes are filtered by confidence threshold\n",
    "        '''\n",
    "        self.gts = gts\n",
    "        self.preds = preds\n",
    "        assert len(gts) == len(preds), '# of images should be the same for predictions and ground truths.'\n",
    "        \n",
    "        cnt_cls = set()\n",
    "        for gts_per_img in self.gts :\n",
    "            for gt_bbox in gts_per_img :\n",
    "                cnt_cls.add(int(gt_bbox[0]))\n",
    "        self.classes = list(cnt_cls)\n",
    "\n",
    "        # convert iou_threshold_range into list\n",
    "        min_threshold, interval, max_threshold  = iou_threshold_range\n",
    "        self.iou_threshold_range = np.linspace(min_threshold, max_threshold, num= int((max_threshold - min_threshold)//interval + 1))\n",
    "        \n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.iou_table_per_img = {imgidx : None for imgidx in range(len(gts))}\n",
    "\n",
    "        # self.total_iou_table = {clslabel : {iou_threshold : None \\\n",
    "        #                                         for iou_threshold in self.iou_threshold_range} \\\n",
    "        #                                             for clslabel in range(num_classes)}\n",
    "\n",
    "        self.TOTAL_TP = [{iou_threshold : 0 for iou_threshold in self.iou_threshold_range} for _ in range(len(self.classes))]\n",
    "        self.TOTAL_FP = [{iou_threshold : 0 for iou_threshold in self.iou_threshold_range} for _ in range(len(self.classes))]\n",
    "        self.TOTAL_FN = [{iou_threshold : 0 for iou_threshold in self.iou_threshold_range} for _ in range(len(self.classes))]\n",
    "\n",
    "        self.total_statistics = []\n",
    "\n",
    "\n",
    "    def calculate_PR(self, imgidx, pred, gt, iou_threshold) :\n",
    "        '''\n",
    "        calculate precision and recall per classes\n",
    "        Recall = TP / (TP+FN)\n",
    "        Precision = TP / (TP + FP)\n",
    "        1. match preds and gts using IoU\n",
    "        2. matched preds will be TP, and remaining unmatched preds will be FP, and unmatched gt are FN.\n",
    "        '''\n",
    "\n",
    "        # bboxes = [[class,x,y,w,h,c],...]. c = confidence_score. filter predicted bboxes by confidence_threshold.\n",
    "        \n",
    "        pred = np.array([p for p in pred if p[-1] > self.confidence_threshold])\n",
    "        \n",
    "        \n",
    "        # 행 : pred, 열 : gt\n",
    "        if self.iou_table_per_img[imgidx] is not None :\n",
    "            iou_table = self.iou_table_per_img[imgidx]\n",
    "        else : \n",
    "            iou_table = torch.zeros((len(pred), len(gt)))\n",
    "            for j, pred_bbox in enumerate(pred) :\n",
    "                for i, gt_bbox in enumerate(gt) :\n",
    "                    iou_table[j][i] = IoU(pred_bbox[..., 1:], gt_bbox[..., 1:])\n",
    "            self.iou_table_per_img[imgidx] = iou_table\n",
    "        # if there are more than one prediction box matched with on gt box, then we choose the predicition with the highest IoU as TP, and treat other matches as FP.     \n",
    "        filtered_iou_table = torch.zeros_like(iou_table)\n",
    "        filtered_iou_table[torch.argmax(iou_table, axis = 0), torch.arange(iou_table.shape[1])] = torch.max(iou_table, axis = 0)[0] # this will leave only one highest IoU per gts\n",
    "        \n",
    "        result = filtered_iou_table > iou_threshold\n",
    "\n",
    "        TP = result.any(axis = 1).sum()\n",
    "        FP = len(pred) - TP\n",
    "        FN = len(gt) - result.any(axis = 0).sum()\n",
    "\n",
    "        return TP.item(), FP.item(), FN.item()\n",
    "\n",
    "    def calculate_average_precision(self, precision, recall) :\n",
    "\n",
    "        precision = list(precision)[:] # [:] = copy list\n",
    "        recall = list(recall)[:]\n",
    "\n",
    "        mean_precision = [0] + precision + [0] \n",
    "        mean_recall = [0] + recall + [0]\n",
    "        \"\"\"\n",
    "        This part makes the precision monotonically decreasing\n",
    "            (goes from the end to the beginning)\n",
    "        \"\"\"\n",
    "        for i in range(len(mean_precision)-2, -1, -1) :\n",
    "            mean_precision[i] = max(mean_precision[i], mean_precision[i+1])\n",
    "        \n",
    "        \"\"\"\n",
    "        This part creates a list of indexes where the recall changes\n",
    "        \"\"\"\n",
    "        i_list = []\n",
    "        for i in range(1, len(mean_recall)) :\n",
    "            if mean_recall[i] != mean_recall[i-1] :\n",
    "                i_list.append(i)\n",
    "        \"\"\"\n",
    "        The Average Precision (AP) is the area under the curve\n",
    "            (numerical integration)\n",
    "        \"\"\"\n",
    "        average_precision = 0.0\n",
    "        for i in i_list :\n",
    "            average_precision += ((mean_recall[i] - mean_recall[i-1]) * mean_precision[i])\n",
    "\n",
    "        # average_precision = torch.mean(torch.trapz(torch.tensor(precision), torch.tensor(recall))).item()\n",
    "        return average_precision\n",
    "\n",
    "\n",
    "    def calculate(self) :\n",
    "        ## TODO : add typing of variables\n",
    "        '''\n",
    "        preds : list of numpy array. []\n",
    "        gts \n",
    "        iou_threshold_range : (minimum_threshold, maximum_threshold, interval)\n",
    "        '''\n",
    "        for imgidx, (pred_by_img, gt_by_img) in enumerate(zip(self.preds, self.gts)) :\n",
    "            for cls_label in self.classes :\n",
    "                cls_preds = pred_by_img[pred_by_img[..., 0] == cls_label]\n",
    "                cls_gts = gt_by_img[gt_by_img[..., 0] == cls_label]\n",
    "\n",
    "                # assert (cls_preds.shape == cls_gts.shape) and (cls_preds.ndim == cls_gts.ndim == 2 and cls_preds.shape[-1] == cls_gts.shape[-1] == 5), \\\n",
    "                #             'pred and gt shape = (# of bboxes over images, len([x,y,w,h,c]) )'\n",
    "\n",
    "                for iou_threshold in self.iou_threshold_range :\n",
    "                    TP, FP, FN = self.calculate_PR(imgidx, cls_preds, cls_gts, iou_threshold)\n",
    "                    self.TOTAL_TP[cls_label][iou_threshold] += TP\n",
    "                    self.TOTAL_FP[cls_label][iou_threshold] += FP\n",
    "                    self.TOTAL_FN[cls_label][iou_threshold] += FN\n",
    "\n",
    "        # calculate Precision and Recall\n",
    "\n",
    "        for cls_label in self.classes :\n",
    "            for iou_threshold in self.iou_threshold_range :\n",
    "                ## round by 3 decimal points\n",
    "                precision = round(self.TOTAL_TP[cls_label][iou_threshold] / (self.TOTAL_TP[cls_label][iou_threshold] + self.TOTAL_FP[cls_label][iou_threshold] + 1e-6), 3) # add 1e-6 to prevent divisionbyzero\n",
    "                recall = round(self.TOTAL_TP[cls_label][iou_threshold] / (self.TOTAL_TP[cls_label][iou_threshold] + self.TOTAL_FN[cls_label][iou_threshold] + 1e-6), 3)\n",
    "\n",
    "                self.total_statistics.append([cls_label, iou_threshold, precision, recall])\n",
    "\n",
    "        self.total_statistics = np.array(self.total_statistics)\n",
    "\n",
    "        # mean average precision = sum(avg_cls_precision) / num_classes. avg_cls_precision = sum of cls_precisions in different recalls / \n",
    "        mean_average_precision = 0\n",
    "        \n",
    "        for cls_label in self.classes :\n",
    "            class_statistics = self.total_statistics[self.total_statistics[..., 0] == cls_label][...,2:4].tolist() # ious x [precision, recall]\n",
    "            class_statistics = sorted(class_statistics, key = lambda x : x[1])\n",
    "            precision, recall = zip(*class_statistics)\n",
    "            \n",
    "            average_precision = self.calculate_average_precision(precision, recall)\n",
    "            mean_average_precision += average_precision\n",
    "            \n",
    "\n",
    "        mean_average_precision /= len(self.classes)\n",
    "\n",
    "        return mean_average_precision\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read sample and test \n",
    "# 00001.txt gt\n",
    "gt1= ['person 0.22 0.22 0.19 0.28 1',\n",
    "'person 0.7475 0.77 0.20500000000000002 0.31 1']\n",
    "# 00001.txt pred\n",
    "pred1 = ['person .88 0.10250000000000001 0.455 0.155 0.24',\n",
    "'person .70 0.6950000000000001 0.7225 0.2 0.335',\n",
    "'person .80 0.7425 0.2125 0.245 0.335',]\n",
    "# 00002.txt gt\n",
    "gt2= ['person 0.7225 0.1925 0.215 0.275 1',\n",
    "'person 0.3375 0.7725 0.295 0.225 1',]\n",
    "# 00002.txt pred\n",
    "pred2 = ['person .71 0.48 0.7000000000000001 0.32 0.29',\n",
    "'person .54 0.28 0.8175 0.3 0.23500000000000001',\n",
    "'person .74 0.2025 0.1775 0.215 0.17500000000000002',]\n",
    "# 00003.txt gt\n",
    "gt3= ['person 0.1675 0.19 0.17500000000000002 0.24 1',\n",
    "'person 0.7375 0.26 0.245 0.22 1',\n",
    "'person 0.6125 0.8125 0.23500000000000001 0.23500000000000001 1',]\n",
    "# 00003.txt pred\n",
    "pred3 = ['person .18 0.7375 0.17250000000000001 0.385 0.195',\n",
    "'person .67 0.545 0.4275 0.23 0.225',\n",
    "'person .38 0.89 0.4425 0.18 0.265',\n",
    "'person .91 0.6425 0.7725 0.23500000000000001 0.23500000000000001',\n",
    "'person .44 0.19 0.85 0.2 0.22',]\n",
    "\n",
    "# 00004.txt gt\n",
    "gt4= ['person 0.365 0.34 0.2 0.26 1',\n",
    "'person 0.8475 0.3 0.155 0.17 1',]\n",
    "# 00004.txt pred\n",
    "pred4 = ['person .35 0.485 0.20500000000000002 0.14 0.13',\n",
    "'person .78 0.245 0.5075000000000001 0.21 0.335',\n",
    "'person .45 0.4975 0.5425 0.125 0.195',\n",
    "'person .14 0.2 0.84 0.3 0.13',]\n",
    "\n",
    "# 00005.txt gt\n",
    "gt5= ['person 0.405 0.28250000000000003 0.22 0.255 1',\n",
    "'person 0.325 0.77 0.17 0.26 1',]\n",
    "# 00005.txt pred\n",
    "pred5 = ['person .62 0.32 0.305 0.14 0.23',\n",
    "'person .44 0.6075 0.125 0.265 0.14',\n",
    "'person .95 0.325 0.7275 0.36 0.145',\n",
    "'person .23 0.325 0.8875000000000001 0.36 0.145',]\n",
    "\n",
    "# 00006.txt gt\n",
    "gt6= ['person 0.31 0.635 0.26 0.38 1',\n",
    "'person 0.42 0.4575 0.22 0.335 1',]\n",
    "# 00006.txt pred\n",
    "pred6 = ['person .45 0.4 0.335 0.37 0.19',\n",
    "'person .84 0.1575 0.8625 0.145 0.17500000000000002',\n",
    "'person .43 0.5375 0.655 0.125 0.21',]\n",
    "\n",
    "# 00007.txt gt\n",
    "gt7= ['person 0.2775 0.3125 0.275 0.315 1',\n",
    "'person 0.41500000000000004 0.48 0.25 0.29 1',]\n",
    "# 00007.txt pred\n",
    "pred7 = ['person .48 0.3325 0.32 0.505 0.44',\n",
    "'person .95 0.2575 0.7025 0.185 0.245',]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {'person' : 0}\n",
    "\n",
    "def convert_bbox(infos) : \n",
    "    bboxes = []\n",
    "    for info in infos :\n",
    "        classlabel, x, y, w, h, c = info.split(' ')\n",
    "        classlabel = class_dict[classlabel]\n",
    "        x,y,w,h,c =  map(float, [x,y,w,h, c])\n",
    "        bboxes.append([classlabel, x, y, w, h, c])\n",
    "    return np.array(bboxes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = [torch.tensor(convert_bbox(g)) for g in [gt1,gt2,gt3,gt4,gt5,gt6,gt7]]\n",
    "preds = [torch.tensor(convert_bbox(p)) for p in [pred1,pred2,pred3,pred4,pred5,pred6,pred7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_metrics = MeanAveragePrecisionMetrics(gts, preds,  iou_threshold_range = (0.2, 0.1, 0.6), confidence_threshold = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6480\\784397417.py:60: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  pred = np.array([p for p in pred if p[-1] > self.confidence_threshold])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6480\\784397417.py:60: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  pred = np.array([p for p in pred if p[-1] > self.confidence_threshold])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06926399999999999"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP_metrics.calculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2       , 0.33333333, 0.46666667, 0.6       ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP_metrics.iou_threshold_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.2       , 0.208     , 0.333     ],\n",
       "       [0.        , 0.33333333, 0.125     , 0.2       ],\n",
       "       [0.        , 0.46666667, 0.125     , 0.2       ],\n",
       "       [0.        , 0.6       , 0.042     , 0.067     ]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP_metrics.total_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015701000000000003"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.042 * 0.067 + 0.125 * 0.133 + 0.208 * 0.133)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((224,224,3))\n",
    "b = torch.randn((224,224,3))\n",
    "c = torch.randn((224,224,3))\n",
    "\n",
    "ls = [a,b,c]\n",
    "\n",
    "torch.stack(ls).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP = MeanAveragePrecision(box_format=\"cxcywh\",\n",
    "            iou_type=\"bbox\",\n",
    "            iou_thresholds=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "preds = [\n",
    "    dict(\n",
    "boxes = torch.randn((batch_size, 4)),\n",
    "scores = torch.randn((batch_size, 1)),\n",
    "labels = torch.tensor([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]).reshape(-1,1),\n",
    ")]\n",
    "gts = [\n",
    "    dict(\n",
    "        boxes = torch.randn((batch_size, 4)),\n",
    "        scores = torch.randn((batch_size, 1)),\n",
    "        labels = torch.tensor([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]).reshape(-1,1),\n",
    "    )\n",
    "]\n",
    "\n",
    "mAP.update(preds, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mAP\u001b[39m.\u001b[39;49mcompute()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchmetrics\\metric.py:530\u001b[0m, in \u001b[0;36mMetric._wrap_compute.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[39m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[39m# if synchronization happened, the current rank accumulated states will be restored to keep\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[39m# accumulation going if ``should_unsync=True``,\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msync_context(\n\u001b[0;32m    526\u001b[0m     dist_sync_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_sync_fn,\n\u001b[0;32m    527\u001b[0m     should_sync\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_sync,\n\u001b[0;32m    528\u001b[0m     should_unsync\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_unsync,\n\u001b[0;32m    529\u001b[0m ):\n\u001b[1;32m--> 530\u001b[0m     value \u001b[39m=\u001b[39m compute(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    531\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_computed \u001b[39m=\u001b[39m _squeeze_if_scalar(value)\n\u001b[0;32m    533\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_computed\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchmetrics\\detection\\mean_ap.py:905\u001b[0m, in \u001b[0;36mMeanAveragePrecision.compute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m    878\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the `Mean-Average-Precision (mAP) and Mean-Average-Recall (mAR)` scores.\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \n\u001b[0;32m    880\u001b[0m \u001b[39m    Note:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[39m        - mar_100_per_class: ``torch.Tensor`` (-1 if class metrics are disabled)\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_classes()\n\u001b[0;32m    906\u001b[0m     precisions, recalls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate(classes)\n\u001b[0;32m    907\u001b[0m     map_val, mar_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summarize_results(precisions, recalls)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchmetrics\\detection\\mean_ap.py:439\u001b[0m, in \u001b[0;36mMeanAveragePrecision._get_classes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a list of unique classes found in ground truth and detection data.\"\"\"\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetection_labels) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroundtruth_labels) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetection_labels \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroundtruth_labels)\u001b[39m.\u001b[39munique()\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m    440\u001b[0m \u001b[39mreturn\u001b[39;00m []\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "mAP.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.tensor([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
